{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from time import time\n",
    "from torch import nn, optim\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "root_random = 50\n",
    "np.random.seed(root_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1\n",
    "\n",
    "- Becoming familiar with tensor based operations and generating synthetic data.\n",
    "- Are input features uninformative? Are the hyper-parameters set correctly? Is there an inherent bug?\n",
    "- Test by passing data with known correct distribution and compare correctness\n",
    "\n",
    "# Problem\n",
    "- This task is a regression task\n",
    "- Use Tensors on Synthetic data I produce\n",
    "- Come up with a mapping function from Rm to Rn, where m and n are set and both are > 3.\n",
    "- Provide the equation of the functions\n",
    "- Add random noise to the output to make the problem more interesting\n",
    "- Ensure underlying function is still recognisable\n",
    "- Be careful of expected value of noise\n",
    "- Pass data to the algorithm to test the output model\n",
    "\n",
    "# Notes\n",
    "- Do not use regression libraries. Must all be implemented using tensors.\n",
    "- Avoid using loops where tensor operations could act on multiple values at once.\n",
    "- Avoid using matrix inverse operators. Use gradients instead.\n",
    "- Be mindful of gradient tracking.\n",
    "- Be sure to generate enough data.\n",
    "- Choose appropriate learning rate\n",
    "- Choose appropriate learning termination criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4.4599,  2.3697,  2.4377,  3.5781],\n",
      "        [ 4.1376, 10.2874,  4.3195,  8.0593],\n",
      "        [ 7.5069,  3.3487,  3.5250,  3.6309],\n",
      "        [ 1.2528,  9.8290,  9.0175,  5.6381],\n",
      "        [ 3.1226,  8.8819,  7.1632,  3.6738],\n",
      "        [ 4.6215,  5.2037,  9.1882,  5.7008],\n",
      "        [ 6.2539,  0.6047,  3.7576,  0.2731],\n",
      "        [ 1.5107,  9.7205,  9.5871,  7.2339],\n",
      "        [ 7.2077,  4.7562,  9.3157,  3.6540],\n",
      "        [ 6.6292,  6.8273,  5.6631,  2.6249],\n",
      "        [ 6.9119,  8.2971,  7.8372,  3.8622],\n",
      "        [ 8.8061,  7.4976,  6.9240,  1.6322],\n",
      "        [ 9.5690,  3.6125,  2.8566,  6.1385],\n",
      "        [ 5.0087,  2.3454,  8.0638,  1.7443],\n",
      "        [ 2.6736,  6.6937,  4.7038,  0.6401],\n",
      "        [ 1.1981,  0.9410,  4.6929,  5.5411],\n",
      "        [ 9.8760,  5.3750,  5.5012,  9.4952],\n",
      "        [ 8.9370,  3.0052,  7.0013, 10.0355],\n",
      "        [ 2.0127,  6.7119,  8.1873,  9.8248],\n",
      "        [ 8.0601,  4.6286,  0.2827,  0.8041]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "#Produce Synthetic Data\n",
    "#Functions \n",
    "#Target (R5) = Input (R4)\n",
    "#Y1 = X1\n",
    "#Y2 = 2*(X2 - X1)\n",
    "#Y3 = X2/(X1 - X4)\n",
    "#Y4 = X3 + X4 - X1\n",
    "#Y5 = X4 - X2 * X3\n",
    "\n",
    "def generate_synthetic_data():\n",
    "    #Generate data\n",
    "    dataset = 10*np.random.random_sample((20, 4))\n",
    "    return dataset\n",
    "\n",
    "def add_noise(data):\n",
    "    adding = np.random.random_sample((20, 4))-0.5\n",
    "    out = np.add(data, adding)\n",
    "            \n",
    "    return out\n",
    "\n",
    "dataset = generate_synthetic_data()\n",
    "results = np.zeros((20, 5))\n",
    "for i in range(20):\n",
    "    for j in range(5):\n",
    "        if j == 0:\n",
    "            results[i][j] = dataset[i][0]\n",
    "        elif j == 1:\n",
    "            results[i][j] = 2 * (dataset[i][1] - dataset[i][0])\n",
    "        elif j == 2:\n",
    "            results[i][j] = dataset[i][1]/(dataset[i][0] - dataset[i][3])\n",
    "        elif j == 3:\n",
    "            results[i][j] = dataset[i][2] + dataset[i][3] - dataset[i][0]\n",
    "        elif j == 4:\n",
    "            results[i][j] = dataset[i][3] - dataset[i][1] * dataset[i][2]\n",
    "        \n",
    "noisey_dataset = add_noise(dataset)\n",
    "noisey_results = np.zeros((20, 5))\n",
    "for i in range(20):\n",
    "    for j in range(5):\n",
    "        if j == 0:\n",
    "            noisey_results[i][j] = noisey_dataset[i][0]\n",
    "        elif j == 1:\n",
    "            noisey_results[i][j] = 2 * (noisey_dataset[i][1] - noisey_dataset[i][0])\n",
    "        elif j == 2:\n",
    "            noisey_results[i][j] = noisey_dataset[i][1]/(noisey_dataset[i][0] - noisey_dataset[i][3])\n",
    "        elif j == 3:\n",
    "            noisey_results[i][j] = noisey_dataset[i][2] + noisey_dataset[i][3] - noisey_dataset[i][0]\n",
    "        elif j == 4:\n",
    "            noisey_results[i][j] = noisey_dataset[i][3] - noisey_dataset[i][1] * noisey_dataset[i][2]\n",
    "\n",
    "#Convert to tensors\n",
    "inputs = torch.from_numpy(noisey_dataset)\n",
    "targets = torch.from_numpy(noisey_results)\n",
    "actual_model = torch.from_numpy(results)\n",
    "print(inputs)\n",
    "\n",
    "#Tensors now ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2985, 0.2469, 0.3170, 0.2870],\n",
      "        [0.8284, 0.7144, 0.3801, 0.4219],\n",
      "        [0.0163, 0.2469, 0.4392, 0.1075],\n",
      "        [0.2914, 0.9250, 0.0121, 0.2378],\n",
      "        [0.7784, 0.6601, 0.0794, 0.1004]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#Regression Model\n",
    "\n",
    "#Weights and Biases\n",
    "weight = torch.rand(5, 4, requires_grad=True)\n",
    "bias = torch.rand(5, requires_grad=True)\n",
    "\n",
    "print(weight)\n",
    "\n",
    "#Model\n",
    "def model(val):\n",
    "    return val @ (weight.t()).double() + bias.double()\n",
    "\n",
    "preds = model(inputs)\n",
    "#print(preds)\n",
    "#print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(561.3101, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(60.5351, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(36.2705, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(25.7885, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(21.0346, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(18.7575, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(17.6030, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(16.9837, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(16.6329, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(16.4232, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(16.2908, dtype=torch.float64, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Loss calculations\n",
    "\n",
    "#Mean square error loss function\n",
    "def mse(pred, target):\n",
    "    diff = pred - target\n",
    "    return torch.sum(diff * diff) / diff.numel()\n",
    "\n",
    "#Calc loss\n",
    "loss = mse(preds, targets)\n",
    "print(loss)\n",
    "\n",
    "#Compute gradient\n",
    "loss.backward()\n",
    "\n",
    "# Train for 100 epochs\n",
    "for i in range(100):  \n",
    "    preds = model(inputs)\n",
    "    loss = mse(preds, targets)\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        weight -= weight.grad * 1e-2\n",
    "        bias -= bias.grad * 1e-2\n",
    "        weight.grad.zero_()\n",
    "        bias.grad.zero_()\n",
    "\n",
    "        if (i+1)%10==0:\n",
    "            print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2\n",
    "\n",
    "- Small scale transfer learning\n",
    "- Train a CNN on a simple problem and try to transfer the learning to ma larger and more difficult problem\n",
    "- USE NN OPTIMISATION FROM pytorch\n",
    "\n",
    "# Problem\n",
    "- Define two network classes\n",
    "    - torchvision.datasets.MNIST\n",
    "    - torchvision.datasets.CIFAR10\n",
    "- Train first network to a good performance\n",
    "- Train second network to observe losses over time (epochs)\n",
    "- Test to see if copying layers from MNIST to CIFAR10 (before training) improves training of the second network.\n",
    "\n",
    "# Notes\n",
    "- Which layers are good to be transfered? (all but last 2?)\n",
    "- How to deal with differing dimensionality?\n",
    "- How to copy objects across?\n",
    "- Presenting findings coherently\n",
    "- Based on https://medium.com/dsnet/training-deep-neural-networks-on-a-gpu-with-pytorch-11079d89805"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3\n",
    "\n",
    "- Option A Cost Sensitive Learning\n",
    "\n",
    "# Problem\n",
    "- C is a k-by-k cost matrix for a classification class with k classes\n",
    "- the element C(i,j) is the cost of classifying an instance of class j as class i\n",
    "- Want to minimise the expected cost of predictions\n",
    "\n",
    "- Write Psuedocode that, when optomised, achieves minimum expected cost of prediction\n",
    "- Assuming a learning algorithm which minimises cost prediction is given, write a different way of achieving minimum expected cost prediction\n",
    "\n",
    "\n",
    "# Notes\n",
    "- Consider cases with k > 1 and briely discuss scalability.\n",
    "- Do not have to use tensors\n",
    "- Can write in pseudo-Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'PATH_TO_STORE_TRAIN_DATA'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-d268445a323b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mDOWNLOAD\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'PATH_TO_STORE_TRAIN_DATA'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mDOWNLOAD\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'PATH_TO_STORE_TRAIN_DATA'"
     ]
    }
   ],
   "source": [
    "DOWNLOAD=False\n",
    "\n",
    "if not(os.path.exists('./data')) or not os.listdir('PATH_TO_STORE_TRAIN_DATA'):\n",
    "    DOWNLOAD=True\n",
    "\n",
    "#Set download=True if MNIST not yet downloaded\n",
    "train_data = MNIST('./data', download=DOWNLOAD, train=True, transform=ToTensor())\n",
    "test_data = MNIST('./data', download=DOWNLOAD, train=False, transform=ToTensor())\n",
    "trainloader = DataLoader(train_data, batch_size=100, shuffle=True)\n",
    "valloader = DataLoader(test_data, batch_size=100, shuffle=True)\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistModel(nn.Module):\n",
    "    \"\"\"Feedfoward neural network with 1 hidden layer\"\"\"\n",
    "    def __init__(self, in_size, hidden_size, out_size):\n",
    "        super().__init__()\n",
    "        # hidden layer\n",
    "        self.linear1 = nn.Linear(in_size, hidden_size)\n",
    "        # output layer\n",
    "        self.linear2 = nn.Linear(hidden_size, out_size)\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        # Flatten the image tensors\n",
    "        xb = xb.view(xb.size(0), -1)\n",
    "        # Get intermediate outputs using hidden layer\n",
    "        out = self.linear1(xb)\n",
    "        # Apply activation function\n",
    "        out = F.relu(out)\n",
    "        # Get predictions using output layer\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "input_size = 784\n",
    "num_classes = 10\n",
    "\n",
    "model = MnistModel(input_size, hidden_size=32, \n",
    "                   out_size=num_classes)\n",
    "for t in model.parameters():\n",
    "    print(t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels in trainloader:\n",
    "    outputs = model(images)\n",
    "    loss = F.cross_entropy(outputs, labels)\n",
    "    print('Loss:', loss.item())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "device = get_default_device()\n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "for images, labels in trainloader:\n",
    "    print(images.shape)\n",
    "    images = to_device(images, device)\n",
    "    print(images.device)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)\n",
    "\n",
    "trainloader = DeviceDataLoader(trainloader, device)\n",
    "valloader = DeviceDataLoader(valloader, device)\n",
    "\n",
    "\n",
    "for xb, yb in valloader:\n",
    "    print('xb.device:', xb.device)\n",
    "    print('yb:', yb)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(model, loss_func, xb, yb, opt=None, metric=None):\n",
    "    # Generate predictions\n",
    "    preds = model(xb)\n",
    "    # Calculate loss\n",
    "    loss = loss_func(preds, yb)\n",
    "                     \n",
    "    if opt is not None:\n",
    "        # Compute gradients\n",
    "        loss.backward()\n",
    "        # Update parameters             \n",
    "        opt.step()\n",
    "        # Reset gradients\n",
    "        opt.zero_grad()\n",
    "    \n",
    "    metric_result = None\n",
    "    if metric is not None:\n",
    "        # Compute the metric\n",
    "        metric_result = metric(preds, yb)\n",
    "    \n",
    "    return loss.item(), len(xb), metric_result\n",
    "\n",
    "def evaluate(model, loss_fn, valloader, metric=None):\n",
    "    with torch.no_grad():\n",
    "        # Pass each batch through the model\n",
    "        results = [loss_batch(model, loss_fn, xb, yb, metric=metric)\n",
    "                   for xb,yb in valloader]\n",
    "        # Separate losses, counts and metrics\n",
    "        losses, nums, metrics = zip(*results)\n",
    "        # Total size of the dataset\n",
    "        total = np.sum(nums)\n",
    "        # Avg. loss across batches \n",
    "        avg_loss = np.sum(np.multiply(losses, nums)) / total\n",
    "        avg_metric = None\n",
    "        if metric is not None:\n",
    "            # Avg. of metric across batches\n",
    "            avg_metric = np.sum(np.multiply(metrics, nums)) / total\n",
    "    return avg_loss, total, avg_metric\n",
    "\n",
    "def fit(epochs, lr, model, loss_fn, train_dl, \n",
    "        valid_dl, metric=None, opt_fn=None):\n",
    "    losses, metrics = [], []\n",
    "    \n",
    "    # Instantiate the optimizer\n",
    "    if opt_fn is None: opt_fn = torch.optim.SGD\n",
    "    opt = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        for xb,yb in train_dl:\n",
    "            loss,_,_ = loss_batch(model, loss_fn, xb, yb, opt)\n",
    "\n",
    "        # Evaluation\n",
    "        result = evaluate(model, loss_fn, valid_dl, metric)\n",
    "        val_loss, total, val_metric = result\n",
    "        \n",
    "        # Record the loss & metric\n",
    "        losses.append(val_loss)\n",
    "        metrics.append(val_metric)\n",
    "        \n",
    "        # Print progress\n",
    "        if metric is None:\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}'\n",
    "                  .format(epoch+1, epochs, val_loss))\n",
    "        else:\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}, {}: {:.4f}'\n",
    "                  .format(epoch+1, epochs, val_loss, \n",
    "                          metric.__name__, val_metric))\n",
    "    return losses, metrics\n",
    "\n",
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.sum(preds == labels).item() / len(preds)\n",
    "\n",
    "# Model (on GPU)\n",
    "model = MnistModel(input_size, hidden_size=32, out_size=num_classes)\n",
    "to_device(model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, total, val_acc = evaluate(model, F.cross_entropy, \n",
    "                                    valloader, metric=accuracy)\n",
    "print('Loss: {:.4f}, Accuracy: {:.4f}'.format(val_loss, val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses1, metrics1 = fit(5, 0.5, model, F.cross_entropy, \n",
    "                        trainloader, valloader, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses2, metrics2 = fit(5, 0.1, model, F.cross_entropy, \n",
    "                        trainloader, valloader, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace these values with your results\n",
    "accuracies = [val_acc] + metrics1 + metrics2\n",
    "plt.plot(accuracies, '-x')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('Accuracy vs. No. of epochs');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
